{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = []\n",
    "for root, dirs, files in os.walk(os.getcwd() + \"/reuters21578/\"):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1] == '.sgm':\n",
    "            filepaths.append(os.path.join(root, file))\n",
    "\n",
    "\n",
    "file_list = [open(file, 'r', encoding='ISO-8859-1') for file in filepaths]\n",
    "soup_list = [BeautifulSoup(file,'lxml') for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics(soup):\n",
    "    tuple_topics = [(topic.parent.get('newid'),i) for topic in soup.find_all('topics') for i in topic.strings]\n",
    "    return tuple_topics\n",
    "\n",
    "def find_texts(soup):\n",
    "    dic_text = {find.parent.get('newid'):find.text.replace(find.title.string if find.parent.title is not None else \"\",\"\").replace(find.dateline.string if find.dateline is not None else \"\",\"\").replace(\"\\n\",\"\") for find in soup.find_all('text') if find.parent.topics.contents!=[]}\n",
    "    return dic_text\n",
    "\n",
    "def get_strs(soup):\n",
    "    topics = find_topics(soup)\n",
    "    text = find_texts(soup)\n",
    "    strs = [topic[1] + \"_label_\" + text.get(topic[0]) for topic in topics]\n",
    "    return strs\n",
    "\n",
    "def write_to_txt(strs):\n",
    "    file = open('raw_y_X.txt','w',encoding='utf-8')\n",
    "    for i in strs:\n",
    "        file.write(i+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs_s = []\n",
    "for soup in soup_list:\n",
    "    strs = get_strs(soup)\n",
    "    for st in strs:\n",
    "        strs_s.append(st)\n",
    "\n",
    "random.shuffle(strs_s)\n",
    "write_to_txt(strs_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_raw = []\n",
    "y_raw = []\n",
    "with open(\"raw_y_X.txt\", \"r\") as infile:\n",
    "    lines = infile.readlines()\n",
    "    for line in lines:\n",
    "        y_raw.append(line.split(\"_label_\")[0])\n",
    "        X_raw.append(line.split(\"_label_\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
    "\n",
    "##################20newsgroups########################\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset=\"train\")\n",
    "X_news = vectorizer.fit_transform(newsgroups_train.data)\n",
    "y_news = newsgroups_train.target\n",
    "\n",
    "##################Reuters###############################\n",
    "\n",
    "X_reuters = vectorizer.fit_transform(X_raw)\n",
    "label_encoder = LabelEncoder()\n",
    "y_reuters = label_encoder.fit_transform(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       128\n",
      "           1       0.88      0.91      0.89       163\n",
      "           2       0.88      0.92      0.90       137\n",
      "           3       0.83      0.79      0.81       136\n",
      "           4       0.92      0.92      0.92       135\n",
      "           5       0.92      0.89      0.91       152\n",
      "           6       0.82      0.90      0.86       146\n",
      "           7       0.92      0.93      0.93       133\n",
      "           8       0.96      0.96      0.96       163\n",
      "           9       0.97      0.95      0.96       134\n",
      "          10       0.93      0.97      0.95       116\n",
      "          11       0.97      0.97      0.97       159\n",
      "          12       0.91      0.90      0.91       139\n",
      "          13       0.99      0.95      0.97       148\n",
      "          14       0.96      0.97      0.97       158\n",
      "          15       0.94      0.98      0.96       167\n",
      "          16       0.94      0.97      0.96       138\n",
      "          17       0.97      0.99      0.98       147\n",
      "          18       0.96      0.87      0.92       124\n",
      "          19       0.99      0.83      0.90       106\n",
      "\n",
      "    accuracy                           0.93      2829\n",
      "   macro avg       0.93      0.93      0.93      2829\n",
      "weighted avg       0.93      0.93      0.93      2829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_news_train, X_news_test, y_news_train, y_news_test = train_test_split(X_news, y_news, test_size=0.25)\n",
    "lsvc_news = LinearSVC(loss=\"squared_hinge\", penalty=\"l2\", C=1, multi_class=\"ovr\")\n",
    "lsvc_news.fit(X_news_train, y_news_train)\n",
    "print(classification_report(y_news_test, lsvc_news.predict(X_news_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.96      0.77       605\n",
      "           1       0.73      0.57      0.64        14\n",
      "           3       0.00      0.00      0.00         9\n",
      "           5       0.35      0.18      0.24        33\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.14      0.19      0.16        16\n",
      "          11       0.67      0.71      0.69        17\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.83      0.72      0.77        40\n",
      "          15       0.67      0.67      0.67        15\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00        64\n",
      "          18       0.00      0.00      0.00         1\n",
      "          20       0.23      0.23      0.23        13\n",
      "          21       0.00      0.00      0.00         1\n",
      "          23       0.56      0.38      0.45        26\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.60      0.49      0.54       173\n",
      "          29       0.00      0.00      0.00        55\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.98      0.92      0.95      1031\n",
      "          32       0.00      0.00      0.00         2\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       0.25      0.18      0.21        11\n",
      "          36       0.30      0.44      0.36        36\n",
      "          37       0.68      0.61      0.64        44\n",
      "          38       0.11      0.16      0.13       148\n",
      "          39       0.00      0.00      0.00         1\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.67      0.60      0.63        10\n",
      "          43       0.00      0.00      0.00         8\n",
      "          44       0.80      0.67      0.73         6\n",
      "          45       1.00      0.17      0.29         6\n",
      "          46       1.00      0.33      0.50         3\n",
      "          47       0.56      0.41      0.47       140\n",
      "          48       0.00      0.00      0.00         2\n",
      "          49       0.60      0.60      0.60        15\n",
      "          50       0.91      0.50      0.65        20\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.72      0.76      0.74        17\n",
      "          53       0.00      0.00      0.00         0\n",
      "          54       0.33      0.14      0.20         7\n",
      "          55       0.67      0.40      0.50         5\n",
      "          57       0.00      0.00      0.00         1\n",
      "          58       0.00      0.00      0.00         2\n",
      "          59       0.00      0.00      0.00         1\n",
      "          60       0.29      0.20      0.24        35\n",
      "          61       0.00      0.00      0.00         3\n",
      "          62       0.20      0.09      0.13        11\n",
      "          63       0.35      0.36      0.36       183\n",
      "          64       0.73      0.49      0.59        55\n",
      "          65       0.00      0.00      0.00         3\n",
      "          66       0.24      0.29      0.26        31\n",
      "          67       0.00      0.00      0.00         2\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.00      0.00      0.00         1\n",
      "          70       0.00      0.00      0.00         2\n",
      "          71       0.17      0.07      0.10        59\n",
      "          72       0.80      0.57      0.67         7\n",
      "          73       0.00      0.00      0.00         1\n",
      "          74       0.00      0.00      0.00        13\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         5\n",
      "          78       0.00      0.00      0.00         2\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         1\n",
      "          82       0.00      0.00      0.00         2\n",
      "          85       0.00      0.00      0.00         1\n",
      "          86       0.20      0.08      0.12        12\n",
      "          87       0.00      0.00      0.00         1\n",
      "          88       0.70      0.35      0.47        20\n",
      "          89       1.00      0.50      0.67         6\n",
      "          90       0.09      0.07      0.08        14\n",
      "          91       0.00      0.00      0.00         1\n",
      "          92       0.83      0.50      0.62        10\n",
      "          94       0.00      0.00      0.00         1\n",
      "          95       0.00      0.00      0.00         0\n",
      "          96       0.00      0.00      0.00         2\n",
      "          97       0.44      0.41      0.43        68\n",
      "          98       0.00      0.00      0.00         7\n",
      "          99       0.00      0.00      0.00         1\n",
      "         100       0.00      0.00      0.00         7\n",
      "         101       0.00      0.00      0.00         5\n",
      "         102       0.00      0.00      0.00         7\n",
      "         103       0.03      0.04      0.04        24\n",
      "         104       0.00      0.00      0.00         3\n",
      "         105       0.50      0.08      0.13        13\n",
      "         106       0.84      0.69      0.76        52\n",
      "         108       0.00      0.00      0.00         4\n",
      "         109       0.00      0.00      0.00         4\n",
      "         110       0.00      0.00      0.00         0\n",
      "         111       0.00      0.00      0.00         4\n",
      "         112       1.00      0.80      0.89        10\n",
      "         113       0.52      0.64      0.57       119\n",
      "         114       0.21      0.25      0.23        36\n",
      "         115       0.03      0.02      0.03        84\n",
      "         116       0.00      0.00      0.00         1\n",
      "         117       0.57      0.57      0.57         7\n",
      "         118       0.00      0.00      0.00        12\n",
      "         119       0.10      0.20      0.13         5\n",
      "\n",
      "    accuracy                           0.61      3576\n",
      "   macro avg       0.25      0.19      0.21      3576\n",
      "weighted avg       0.60      0.61      0.59      3576\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangyifan/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/yangyifan/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_reuters_train, X_reuters_test, y_reuters_train, y_reuters_test = train_test_split(X_reuters, y_reuters, test_size=0.25)\n",
    "lsvc_reuters = LinearSVC(loss=\"squared_hinge\", penalty=\"l2\", C=1, multi_class=\"ovr\")\n",
    "lsvc_reuters.fit(X_reuters_train, y_reuters_train)\n",
    "print(classification_report(y_reuters_test, lsvc_reuters.predict(X_reuters_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       128\n",
      "           1       0.91      0.82      0.86       163\n",
      "           2       0.84      0.86      0.85       137\n",
      "           3       0.76      0.75      0.76       136\n",
      "           4       0.94      0.89      0.92       135\n",
      "           5       0.92      0.89      0.91       152\n",
      "           6       0.88      0.81      0.84       146\n",
      "           7       0.86      0.92      0.89       133\n",
      "           8       0.95      0.95      0.95       163\n",
      "           9       0.91      0.94      0.93       134\n",
      "          10       0.86      0.97      0.91       116\n",
      "          11       0.95      0.98      0.97       159\n",
      "          12       0.91      0.83      0.87       139\n",
      "          13       0.96      0.92      0.94       148\n",
      "          14       0.93      0.97      0.95       158\n",
      "          15       0.91      0.97      0.94       167\n",
      "          16       0.87      0.97      0.92       138\n",
      "          17       0.91      0.99      0.95       147\n",
      "          18       0.90      0.84      0.87       124\n",
      "          19       0.95      0.70      0.80       106\n",
      "\n",
      "    accuracy                           0.90      2829\n",
      "   macro avg       0.90      0.90      0.90      2829\n",
      "weighted avg       0.90      0.90      0.90      2829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnb_news = ComplementNB(alpha=1)\n",
    "cnb_news.fit(X_news_train, y_news_train)\n",
    "print(classification_report(y_news_test, cnb_news.predict(X_news_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.96      0.77       605\n",
      "           1       0.88      0.50      0.64        14\n",
      "           3       0.00      0.00      0.00         9\n",
      "           5       0.38      0.09      0.15        33\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.07      0.06      0.06        16\n",
      "          11       0.67      0.59      0.62        17\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.84      0.68      0.75        40\n",
      "          15       0.64      0.60      0.62        15\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00        64\n",
      "          18       0.00      0.00      0.00         1\n",
      "          20       0.25      0.08      0.12        13\n",
      "          21       0.00      0.00      0.00         1\n",
      "          23       0.38      0.35      0.36        26\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.64      0.51      0.57       173\n",
      "          29       0.00      0.00      0.00        55\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.83      0.93      0.88      1031\n",
      "          32       0.00      0.00      0.00         2\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       1.00      0.14      0.25         7\n",
      "          35       0.20      0.09      0.13        11\n",
      "          36       0.27      0.33      0.30        36\n",
      "          37       0.69      0.50      0.58        44\n",
      "          38       0.13      0.22      0.16       148\n",
      "          39       0.00      0.00      0.00         1\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.71      0.50      0.59        10\n",
      "          43       0.00      0.00      0.00         8\n",
      "          44       0.75      0.50      0.60         6\n",
      "          45       0.00      0.00      0.00         6\n",
      "          46       0.00      0.00      0.00         3\n",
      "          47       0.61      0.27      0.38       140\n",
      "          48       0.00      0.00      0.00         2\n",
      "          49       0.60      0.40      0.48        15\n",
      "          50       0.80      0.20      0.32        20\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.71      0.29      0.42        17\n",
      "          53       0.00      0.00      0.00         0\n",
      "          54       0.50      0.14      0.22         7\n",
      "          55       0.50      0.20      0.29         5\n",
      "          57       0.00      0.00      0.00         1\n",
      "          58       0.00      0.00      0.00         2\n",
      "          59       0.00      0.00      0.00         1\n",
      "          60       0.18      0.11      0.14        35\n",
      "          61       0.00      0.00      0.00         3\n",
      "          62       0.00      0.00      0.00        11\n",
      "          63       0.37      0.51      0.43       183\n",
      "          64       0.82      0.33      0.47        55\n",
      "          65       0.00      0.00      0.00         3\n",
      "          66       0.18      0.13      0.15        31\n",
      "          67       0.00      0.00      0.00         2\n",
      "          69       0.00      0.00      0.00         1\n",
      "          70       0.00      0.00      0.00         2\n",
      "          71       0.14      0.05      0.07        59\n",
      "          72       1.00      0.57      0.73         7\n",
      "          73       0.00      0.00      0.00         1\n",
      "          74       0.00      0.00      0.00        13\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         5\n",
      "          78       0.00      0.00      0.00         2\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         1\n",
      "          82       0.00      0.00      0.00         2\n",
      "          85       0.00      0.00      0.00         1\n",
      "          86       0.20      0.08      0.12        12\n",
      "          87       0.00      0.00      0.00         1\n",
      "          88       0.00      0.00      0.00        20\n",
      "          89       0.00      0.00      0.00         6\n",
      "          90       0.00      0.00      0.00        14\n",
      "          91       0.00      0.00      0.00         1\n",
      "          92       0.80      0.40      0.53        10\n",
      "          94       0.00      0.00      0.00         1\n",
      "          95       0.00      0.00      0.00         0\n",
      "          96       0.00      0.00      0.00         2\n",
      "          97       0.46      0.40      0.43        68\n",
      "          98       0.00      0.00      0.00         7\n",
      "          99       0.00      0.00      0.00         1\n",
      "         100       0.00      0.00      0.00         7\n",
      "         101       0.00      0.00      0.00         5\n",
      "         102       0.00      0.00      0.00         7\n",
      "         103       0.00      0.00      0.00        24\n",
      "         104       0.00      0.00      0.00         3\n",
      "         105       1.00      0.08      0.14        13\n",
      "         106       0.80      0.54      0.64        52\n",
      "         108       0.00      0.00      0.00         4\n",
      "         109       0.00      0.00      0.00         4\n",
      "         110       0.00      0.00      0.00         0\n",
      "         111       0.00      0.00      0.00         4\n",
      "         112       1.00      0.60      0.75        10\n",
      "         113       0.43      0.60      0.50       119\n",
      "         114       0.17      0.14      0.15        36\n",
      "         115       0.00      0.00      0.00        84\n",
      "         116       0.00      0.00      0.00         1\n",
      "         117       0.50      0.14      0.22         7\n",
      "         118       0.00      0.00      0.00        12\n",
      "         119       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.58      3576\n",
      "   macro avg       0.21      0.14      0.15      3576\n",
      "weighted avg       0.55      0.58      0.55      3576\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangyifan/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/yangyifan/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cnb_reuters = ComplementNB(alpha=1)\n",
    "cnb_reuters.fit(X_reuters_train, y_reuters_train)\n",
    "print(classification_report(y_reuters_test, cnb_reuters.predict(X_reuters_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Preprocessing**\n",
    "\n",
    "I used BeautifulSoup to parse the Reuters data instead of Regular Expressions, which turned out to be more difficult than expected.\n",
    "When parsing the data, only documents with the label \\\\<TOPICS\\> were chosen, and texts were read in by stripping the title and dateline information.\n",
    "The \"newid\" information makes the text and its topic match.\n",
    "The X_raw would be all the texts, and the y_raw would be their corresponding topics.\n",
    "The topic and text of every document were stored in a .txt file with a \\_label_ mark separating them.\n",
    "\n",
    "Then TF-IDF vectorizer helped with the encoding of both datasets with unigram and bigram features excluding the stop words. Label Encoder of sklearn was used to encode target labels with value between 0 and n_classes-1.\n",
    "\n",
    "**Model Selection**\n",
    "\n",
    "Two models were implemented, Linear Support Vector Classifier as the non-probabilistic one and Complement Naive Bayes as the probabilistic one. \n",
    "LinearSVC has more flexibility in the choice of penalties and loss functions, and Complement NB is suitable for imbalanced datasets, in our case, the Reuters dataset. The inductive bias of a SVM is that distinct classes tend to be separated by wide margins (maximum margin).\n",
    "The naive bayes classifier assumes that the inputs are independent of each other, and the input only depends on the output label.\n",
    "\n",
    "The train and test sets were split by sklearn. I tried several test sizes and found out that the performance did not vary a lot, and I went with 0.25.\n",
    "\n",
    "For hyperparameters, I chose to use the default values for both classifiers.\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "The evaluation metric I chose was classification report in sklearn. It shows the accuracy, recall, and F1 score for each label and overall.\n",
    "The results I got from the experiments showed that the 20 newsgroups dataset outperformed Reuters a lot on both classifiers.\n",
    "The overall accuracy for 20 newsgroups dataset was about 0.9, while the overall accuracy for Reuters was around 0.6.\n",
    "When classifying the Reuters dataset, labels with high frequency were predicted with higher accuracy and recall, whereas rare labels got nearly 0. \n",
    "I think it is because the Reuters dataset is not as well-formed as the other one. There's a lot of \"noise\" in the texts (e.g. many texts are like \\*\\*\\*\\*\\*Blah Blah Blah).\n",
    "\n",
    "Also, the 20 newsgroups dataset only has 20 labels whereas the Reuters dataset has 120 labels, making the classification task harder. With so many labels to classify, more input data is needed for the chosen model to learn.\n",
    "\n",
    "As for the warining of zero division, there are zero accuracies and recalls so it is natural to see that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}